{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome Summary Datafunctions is a library of stored procedures for Google BigQuery, which abstract common data manipulation patterns into simple atomic, composable actions. They can be used for single step transformations, extended and chained together into more complex sequential transformation pipelines. Datafunctions are written in Dynamic SQL and executed entirely within BigQuery. Status Datafunctions are currently in active development, with core functions being extended and tested, and additional profiling, monitoring and QA functionality being developed and built into the library. The library is currently in private beta. To request access, please click the button below and fill out a very quick form: Request Access","title":"Welcome"},{"location":"#welcome","text":"","title":"Welcome"},{"location":"#summary","text":"Datafunctions is a library of stored procedures for Google BigQuery, which abstract common data manipulation patterns into simple atomic, composable actions. They can be used for single step transformations, extended and chained together into more complex sequential transformation pipelines. Datafunctions are written in Dynamic SQL and executed entirely within BigQuery.","title":"Summary"},{"location":"#status","text":"Datafunctions are currently in active development, with core functions being extended and tested, and additional profiling, monitoring and QA functionality being developed and built into the library. The library is currently in private beta. To request access, please click the button below and fill out a very quick form: Request Access","title":"Status"},{"location":"advanced_functions/","text":"Advanced Functions Summary Advanced functions require more complex configuration, but enable users to leverage the power of an advanced set of Google Cloud Platform API functions. By using a combination of simple or complex chains of datafunctions, scheduled queries, pub/sub triggers and cloud functions, new or backlog data can be augmented with responses from any number of Google or external APIs. Example flow: Google APIs Geospatial BigQuery GIS Geocoding Distance Matrix Places Machine Learning BigQuery ML Cloud Vision Cloud Speech to Text Translation Cloud Natural Language External APIs Custom integrations to external APIs can potantially also be developed according to requirements.","title":"Advanced functions"},{"location":"advanced_functions/#advanced-functions","text":"","title":"Advanced Functions"},{"location":"advanced_functions/#summary","text":"Advanced functions require more complex configuration, but enable users to leverage the power of an advanced set of Google Cloud Platform API functions. By using a combination of simple or complex chains of datafunctions, scheduled queries, pub/sub triggers and cloud functions, new or backlog data can be augmented with responses from any number of Google or external APIs. Example flow:","title":"Summary"},{"location":"advanced_functions/#google-apis","text":"","title":"Google APIs"},{"location":"advanced_functions/#geospatial","text":"","title":"Geospatial"},{"location":"advanced_functions/#bigquery-gis","text":"","title":"BigQuery GIS"},{"location":"advanced_functions/#geocoding","text":"","title":"Geocoding"},{"location":"advanced_functions/#distance-matrix","text":"","title":"Distance Matrix"},{"location":"advanced_functions/#places","text":"","title":"Places"},{"location":"advanced_functions/#machine-learning","text":"","title":"Machine Learning"},{"location":"advanced_functions/#bigquery-ml","text":"","title":"BigQuery ML"},{"location":"advanced_functions/#cloud-vision","text":"","title":"Cloud Vision"},{"location":"advanced_functions/#cloud-speech-to-text","text":"","title":"Cloud Speech to Text"},{"location":"advanced_functions/#translation","text":"","title":"Translation"},{"location":"advanced_functions/#cloud-natural-language","text":"","title":"Cloud Natural Language"},{"location":"advanced_functions/#external-apis","text":"Custom integrations to external APIs can potantially also be developed according to requirements.","title":"External APIs"},{"location":"cloud_functions/","text":"Cloud functions main.py requirements.txt #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; }","title":"Cloud functions"},{"location":"cloud_functions/#cloud-functions","text":"main.py requirements.txt #include <iostream> int main(void) { std::cout << \"Hello world!\" << std::endl; return 0; }","title":"Cloud functions"},{"location":"core_functions/","text":"Core Functions These functions are discrete, composable data manipulation actions. They are grouped into similar function sets and are called using the following syntax: datafunctions.function_set.function_name(arguments) datafunctions.cleanse label_duplicate_rows Argument Type Description source_ref STRING IN Source table or view reference destination_view_ref STRING IN Destination view reference unique_identifiers ARRAY<STRING> IN Column names with expected unique field combination CALL datafunctions . cleanse . label_duplicate_rows ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING [] # unique_identifiers ARRAY < STRING > ); Output: A view at destination_view_ref with the data in the source_ref table or view, and an additional field duplicate_label : New Column Value Description duplicate_label unique Row is unique for column values defined in unique_identifiers duplicate_label duplicate Row is duplicated for column values defined in unique_identifiers deduplicate replace_substrings input_string STRING, replace_substring_rules ARRAY<STRUCT<substring STRING, replacement STRING>>, OUT output_string STRING datafunctions.compute cumulative_sum source_ref STRING, destination_view_ref STRING, value_column STRING, partition_columns ARRAY<STRING>, sort_columns ARRAY<STRING>, cumulative_field_name STRING daily_moving_average CALL datafunctions . compute . daily_moving_average ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING '' , # value_column STRING [], # partition_columns ARRAY < STRING > [], # sort_columns ARRAY < STRING > 'ASC/DESC' , # asc_or_desc STRING 7 # moving_average_days INT64 ); source_ref STRING, destination_view_ref STRING, value_column STRING, partition_columns ARRAY , sort_columns ARRAY , moving_average_days INT64 datafunctions.profile count_column_empty_values source_ref STRING, column_name STRING, OUT empty_column_summary STRUCT<row_count INT64, empty_record_count INT64, empty_record_ratio FLOAT64> count_column_null_values source_ref STRING, column_name STRING, OUT column_null_summary STRUCT<row_count INT64, null_record_count INT64, null_record_ratio FLOAT64> get_column_cardinality source_ref STRING, column_name STRING, OUT column_cardinality INT64 get_data_shape source_ref STRING, OUT data_shape STRUCT<column_count INT64, row_count INT64> datafunctions.select all source_ref STRING, destination_view_ref STRING all_except source_ref STRING, destination_view_ref STRING, except_sql ARRAY<STRING>, where_sql ARRAY<STRING> all_except_where all_where CALL datafunctions . select . all_where ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING [] # where_sql ARRAY < STRING > ); columns_where source_ref STRING, destination_view_ref STRING, columns_sql ARRAY<STRING>, where_sql ARRAY<STRING> custom_where source_ref STRING, destination_view_ref STRING, columns_sql ARRAY<STRING>, custom_columns_sql ARRAY<STRING>, where_sql ARRAY<STRING> sql CALL datafunctions . select . sql ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING '' , # select_line STRING [], # columns_sql ARRAY < STRING > [], # except_sql ARRAY < STRING > [], # replace_sql ARRAY < STRING > [], # custom_columns_sql ARRAY < STRING > [], # where_sql ARRAY < STRING > [] # groupby_sql ARRAY < STRING > ); datafunctions.transform fill_in_date_gaps CALL datafunctions . transform . fill_in_date_gaps ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING '' # date_column STRING ); pivot CALL datafunctions . transform . pivot ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING [], # dimensions ARRAY < STRING > , '' , # metric STRING , '' , # pivot_field STRING '_' # new_column_name_delimiter STRING ); unpivot source_ref STRING, destination_view STRING, dimensions ARRAY<STRING>, unpivot_columns ARRAY<STRING> left_join source_ref_left STRING, source_ref_right STRING, destination_view STRING, left_join_fields ARRAY<STRING>, right_join_fields ARRAY<STRING> safe_left_join source_ref_left STRING, source_ref_right STRING, destination_view STRING, left_join_fields ARRAY<STRING>, right_join_fields ARRAY<STRING> datafunctions.utilities get_columns source_ref STRING, OUT columns_out ARRAY<STRING> get_min_or_max_column_value CALL datafunctions . utilities . get_min_or_max_column_value ( '' , # source_ref STRING '' , # column_name STRING 'MIN/MAX' , # min_or_max STRING VARIABLE , # OUT min_max_value ANY TYPE 'STRING/DATE/INT64/ETC' # output_type STRING ); get_unique_column_values source_ref STRING, column_name STRING, OUT column_values ARRAY<STRING> add_row_hash_columns_md5 source_ref STRING, destination_view_ref STRING, include_columns ARRAY<STRING>, row_hash_name STRING add_row_hash_md5 source_ref STRING, destination_view_ref STRING, row_hash_name STRING build_gapless_day_array source_ref STRING, day_column STRING, OUT day_array ARRAY<DATE>","title":"Core Functions"},{"location":"core_functions/#core-functions","text":"These functions are discrete, composable data manipulation actions. They are grouped into similar function sets and are called using the following syntax: datafunctions.function_set.function_name(arguments)","title":"Core Functions"},{"location":"core_functions/#datafunctionscleanse","text":"","title":"datafunctions.cleanse"},{"location":"core_functions/#label_duplicate_rows","text":"Argument Type Description source_ref STRING IN Source table or view reference destination_view_ref STRING IN Destination view reference unique_identifiers ARRAY<STRING> IN Column names with expected unique field combination CALL datafunctions . cleanse . label_duplicate_rows ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING [] # unique_identifiers ARRAY < STRING > ); Output: A view at destination_view_ref with the data in the source_ref table or view, and an additional field duplicate_label : New Column Value Description duplicate_label unique Row is unique for column values defined in unique_identifiers duplicate_label duplicate Row is duplicated for column values defined in unique_identifiers","title":"label_duplicate_rows"},{"location":"core_functions/#deduplicate","text":"","title":"deduplicate"},{"location":"core_functions/#replace_substrings","text":"input_string STRING, replace_substring_rules ARRAY<STRUCT<substring STRING, replacement STRING>>, OUT output_string STRING","title":"replace_substrings"},{"location":"core_functions/#datafunctionscompute","text":"","title":"datafunctions.compute"},{"location":"core_functions/#cumulative_sum","text":"source_ref STRING, destination_view_ref STRING, value_column STRING, partition_columns ARRAY<STRING>, sort_columns ARRAY<STRING>, cumulative_field_name STRING","title":"cumulative_sum"},{"location":"core_functions/#daily_moving_average","text":"CALL datafunctions . compute . daily_moving_average ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING '' , # value_column STRING [], # partition_columns ARRAY < STRING > [], # sort_columns ARRAY < STRING > 'ASC/DESC' , # asc_or_desc STRING 7 # moving_average_days INT64 ); source_ref STRING, destination_view_ref STRING, value_column STRING, partition_columns ARRAY , sort_columns ARRAY , moving_average_days INT64","title":"daily_moving_average"},{"location":"core_functions/#datafunctionsprofile","text":"","title":"datafunctions.profile"},{"location":"core_functions/#count_column_empty_values","text":"source_ref STRING, column_name STRING, OUT empty_column_summary STRUCT<row_count INT64, empty_record_count INT64, empty_record_ratio FLOAT64>","title":"count_column_empty_values"},{"location":"core_functions/#count_column_null_values","text":"source_ref STRING, column_name STRING, OUT column_null_summary STRUCT<row_count INT64, null_record_count INT64, null_record_ratio FLOAT64>","title":"count_column_null_values"},{"location":"core_functions/#get_column_cardinality","text":"source_ref STRING, column_name STRING, OUT column_cardinality INT64","title":"get_column_cardinality"},{"location":"core_functions/#get_data_shape","text":"source_ref STRING, OUT data_shape STRUCT<column_count INT64, row_count INT64>","title":"get_data_shape"},{"location":"core_functions/#datafunctionsselect","text":"","title":"datafunctions.select"},{"location":"core_functions/#all","text":"source_ref STRING, destination_view_ref STRING","title":"all"},{"location":"core_functions/#all_except","text":"source_ref STRING, destination_view_ref STRING, except_sql ARRAY<STRING>, where_sql ARRAY<STRING>","title":"all_except"},{"location":"core_functions/#all_except_where","text":"","title":"all_except_where"},{"location":"core_functions/#all_where","text":"CALL datafunctions . select . all_where ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING [] # where_sql ARRAY < STRING > );","title":"all_where"},{"location":"core_functions/#columns_where","text":"source_ref STRING, destination_view_ref STRING, columns_sql ARRAY<STRING>, where_sql ARRAY<STRING>","title":"columns_where"},{"location":"core_functions/#custom_where","text":"source_ref STRING, destination_view_ref STRING, columns_sql ARRAY<STRING>, custom_columns_sql ARRAY<STRING>, where_sql ARRAY<STRING>","title":"custom_where"},{"location":"core_functions/#sql","text":"CALL datafunctions . select . sql ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING '' , # select_line STRING [], # columns_sql ARRAY < STRING > [], # except_sql ARRAY < STRING > [], # replace_sql ARRAY < STRING > [], # custom_columns_sql ARRAY < STRING > [], # where_sql ARRAY < STRING > [] # groupby_sql ARRAY < STRING > );","title":"sql"},{"location":"core_functions/#datafunctionstransform","text":"","title":"datafunctions.transform"},{"location":"core_functions/#fill_in_date_gaps","text":"CALL datafunctions . transform . fill_in_date_gaps ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING '' # date_column STRING );","title":"fill_in_date_gaps"},{"location":"core_functions/#pivot","text":"CALL datafunctions . transform . pivot ( 'project_id.dataset_id.source_name' , # source_ref STRING 'project_id.dataset_id.destination_view_name' , # destination_view_ref STRING [], # dimensions ARRAY < STRING > , '' , # metric STRING , '' , # pivot_field STRING '_' # new_column_name_delimiter STRING );","title":"pivot"},{"location":"core_functions/#unpivot","text":"source_ref STRING, destination_view STRING, dimensions ARRAY<STRING>, unpivot_columns ARRAY<STRING>","title":"unpivot"},{"location":"core_functions/#left_join","text":"source_ref_left STRING, source_ref_right STRING, destination_view STRING, left_join_fields ARRAY<STRING>, right_join_fields ARRAY<STRING>","title":"left_join"},{"location":"core_functions/#safe_left_join","text":"source_ref_left STRING, source_ref_right STRING, destination_view STRING, left_join_fields ARRAY<STRING>, right_join_fields ARRAY<STRING>","title":"safe_left_join"},{"location":"core_functions/#datafunctionsutilities","text":"","title":"datafunctions.utilities"},{"location":"core_functions/#get_columns","text":"source_ref STRING, OUT columns_out ARRAY<STRING>","title":"get_columns"},{"location":"core_functions/#get_min_or_max_column_value","text":"CALL datafunctions . utilities . get_min_or_max_column_value ( '' , # source_ref STRING '' , # column_name STRING 'MIN/MAX' , # min_or_max STRING VARIABLE , # OUT min_max_value ANY TYPE 'STRING/DATE/INT64/ETC' # output_type STRING );","title":"get_min_or_max_column_value"},{"location":"core_functions/#get_unique_column_values","text":"source_ref STRING, column_name STRING, OUT column_values ARRAY<STRING>","title":"get_unique_column_values"},{"location":"core_functions/#add_row_hash_columns_md5","text":"source_ref STRING, destination_view_ref STRING, include_columns ARRAY<STRING>, row_hash_name STRING","title":"add_row_hash_columns_md5"},{"location":"core_functions/#add_row_hash_md5","text":"source_ref STRING, destination_view_ref STRING, row_hash_name STRING","title":"add_row_hash_md5"},{"location":"core_functions/#build_gapless_day_array","text":"source_ref STRING, day_column STRING, OUT day_array ARRAY<DATE>","title":"build_gapless_day_array"},{"location":"python_package/","text":"Python package (dfio) We use and maintain a python package on PyPi called dfio . In order to use any of the modules and functions, simply pip install in your working (preferably virtual) environment: pip install dfio These functions are designed to be used in deployed Cloud Functions, so appropriate permissions should be attached to the associated service account used for execution. Environment Variables (local environment) Local execution and testing will require setting the following environment variables in your IDE: GCP_PROJECT Description: Current project_id GOOGLE_APPLICATION_CREDENTIALS Description: Relative path to credentials JSON file with appropriate permissions dfio.utilities Utility functions: get_local_json_file_as_dict dfio.gcs Functions for interacting with Google Cloud Storage upload_url_to_gcs dfio.testing Functions for seamless local testing of Cloud Functions execute_local_cf_test_with_pubsub_payload Configurations In the terminal: pip install dfconfig Google Cloud Storage In the terminal: pip install dfgcs","title":"Python package (dfio)"},{"location":"python_package/#python-package-dfio","text":"We use and maintain a python package on PyPi called dfio . In order to use any of the modules and functions, simply pip install in your working (preferably virtual) environment: pip install dfio These functions are designed to be used in deployed Cloud Functions, so appropriate permissions should be attached to the associated service account used for execution.","title":"Python package (dfio)"},{"location":"python_package/#environment-variables-local-environment","text":"Local execution and testing will require setting the following environment variables in your IDE: GCP_PROJECT Description: Current project_id GOOGLE_APPLICATION_CREDENTIALS Description: Relative path to credentials JSON file with appropriate permissions","title":"Environment Variables (local environment)"},{"location":"python_package/#dfioutilities","text":"Utility functions:","title":"dfio.utilities"},{"location":"python_package/#get_local_json_file_as_dict","text":"","title":"get_local_json_file_as_dict"},{"location":"python_package/#dfiogcs","text":"Functions for interacting with Google Cloud Storage","title":"dfio.gcs"},{"location":"python_package/#upload_url_to_gcs","text":"","title":"upload_url_to_gcs"},{"location":"python_package/#dfiotesting","text":"Functions for seamless local testing of Cloud Functions","title":"dfio.testing"},{"location":"python_package/#execute_local_cf_test_with_pubsub_payload","text":"","title":"execute_local_cf_test_with_pubsub_payload"},{"location":"python_package/#configurations","text":"In the terminal: pip install dfconfig","title":"Configurations"},{"location":"python_package/#google-cloud-storage","text":"In the terminal: pip install dfgcs","title":"Google Cloud Storage"},{"location":"tutorials/","text":"Tutorials","title":"Tutorials"},{"location":"tutorials/#tutorials","text":"","title":"Tutorials"}]}